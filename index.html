<!DOCTYPE html>
<!-- Wallpaper.png old wallpaper--> 
<html lang="en">
<head>
  <title>Alex Portfolio</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <style>
    body { font-family: "Lato", sans-serif }
    .mySlides { display: none }
    /* Add a class for the wallpaper section */
    .wallpaper-section {
      background-image: url('Wallpaper.jpg'); 
      background-size: cover;
      color: white;
      text-align: center;
      padding: 64px 16px;
    }
    /* Style for the h2 in the wallpaper section */
    .wallpaper-section h2 {
      margin-bottom: 1px;
    }
    /* Style for the p in the wallpaper section */
    .wallpaper-section p {
      margin-bottom: 1px;
    }
    /* Style for the image in the wallpaper section */
    .wallpaper-section img {
      width: 100%;
      max-width: 800px;
      margin-top: 20px;
    }

    .colored-band {
      height: 40px; /* Adjust the height as needed */
      background-color: #a31f82; /* Set the desired background color */
    }

  </style>
</head>
<body>


<!-- Wallpaper -->
<div class="wallpaper-section" style="text-align: center;">

  <div style="display: inline-block; text-align: center; vertical-align: top; max-width: 800px; margin: 0 auto;">
    <h2 class="w3-wide">ALEX SANTINI PORTFOLIO</h2>
    <p class="w3-opacity"><i>MSc in Data science with experience in Machine learning, neural networks, Data visualization and Big data technologies looking for making an impact as a data scientist</i></p>
    <img src="image.png" alt="Image Description">
    <p class="w3-justify"> Linkedin: <a href="https://www.linkedin.com/in/alex-santini-745546201/" target="_blank">Alex Santini</a> &nbsp;&nbsp; &nbsp;  |  &nbsp; &nbsp; &nbsp;
      Github: <a href="https://github.com/alexCCTcollege" target="_blank">Alex Github</a> &nbsp; &nbsp;&nbsp; |  &nbsp;&nbsp; &nbsp;
      Phone: +353 894898821  &nbsp;&nbsp;&nbsp; |    &nbsp;&nbsp;&nbsp;
      Email: <a href="mailto:santini.alex.urbino@gmail.com" target="_blank">santini.alex.urbino@gmail.com</a> 
    </p>  
  </div>

  <div style="clear: both;"></div>
</div>



<!-- Navbar -->
<div class="w3-top">
  <div class="w3-bar w3-black w3-card">
    <a class="w3-bar-item w3-button w3-padding-large w3-hide-medium w3-hide-large w3-right" href="javascript:void(0)" onclick="myFunction()" title="Toggle Navigation Menu"><i class="fa fa-bars"></i></a>
    <a href="#" class="w3-bar-item w3-button w3-padding-large">HOME</a>
    <a href="#band" class="w3-bar-item w3-button w3-padding-large w3-hide-small">Big Data</a>
    <a href="#band_2" class="w3-bar-item w3-button w3-padding-large w3-hide-small">Machine learning</a>
    <a href="#band_3" class="w3-bar-item w3-button w3-padding-large w3-hide-small">Data visualization</a>
    <a href="#band_4" class="w3-bar-item w3-button w3-padding-large w3-hide-small">Up next...</a>
    </div>
  </div>
</div>

<!-- Page content -->
<div class="w3-content" style="max-width:2000px;margin-top:46px">

  <!-- project 1 section -->
  <div class="w3-container w3-content w3-center w3-padding-64" style="max-width:800px" id="band">
    <h2 class="w3-wide">Sentiment analysis using Pyspark, MySQL and Cassandra</h2>
    <p class="w3-opacity"><i>Big Data</i></p>
    <p class="w3-justify">In this project we explore how to extract the sentiment of more than one million tweets, 
      understanding the sentiment of such a vast number of tweets involves various challenges and complexities. 
      The rich and dynamic nature of Twitter data, characterized by abbreviations, slang, and context-specific language, 
      demands a robust approach to sentiment analysis. My aim is to delve into the intricate task of deciphering the emotions 
      expressed within this massive corpus of tweets, providing valuable insights into public opinion and trends. <br> 
      In order to process such vast amount of data I'll be using Pyspark as distributed processing system which is going to be link to a Mysql Database for
      storing the raw data and a Cassandra database for storing the sentiment dataset; the raw data is stored as csv file.  </p>

    <!-- Add an image here -->
    <img src="csv.png" alt="Raw data in csv file" style="width: 100%; max-width: 800px; margin-top: 20px;">

    <p class="w3-justify">Pyspark will be used as main processing tool, the implementation will link to the databases using jar and database connectors which has been installed already,
      this implementation strategy will allow to read and write to the two databases using pyspark, full code here: <a href="https://github.com/alexCCTcollege/Time-series-using-Pyspark-Cassandra-Mysql/blob/main/Spark-Mysql-Cassandra.ipynb" target="_blank">Spark-cassandra-processing</a> </p>

    <!-- Add an image here -->
    <img src="diagramma.png" alt="Implementation" style="width: 100%; max-width: 400px; margin-top: 20px;">

    <p class="w3-justify">Once data has been loaded to Pyspark the following processes have been completed: <br><br>
    - Cleaning: removal of hashtags, tags and general text Cleaning <br>
    - Datetime formatting: checking and extracting correct date from tweets <br>
    - Sentiment analysis: using Vader and Text blob algorithms I've extracted the sentiment of every tweet and compared the two <br>
    - Aggregate the data by day in order to have the average sentiment of Tweets per day  <br><br>

    Vader and TextBlob are tools intended for uses on different datasets in this case Vader was able to detect non neutral sentiment better than text blob </p>
    
    <img src="blob.png" alt="Sentiment" style="width: 100%; max-width: 600px; margin-top: 20px;">

    <img src="bocplot.png" alt="Sentiment" style="width: 100%; max-width: 600px; margin-top: 20px;">

    <p class="w3-justify">The end product of the all process was an aggregated dataset with dates and average sentiment by day, because of the missing data I had to interpolate the missing data points by using
    an average seasonal mean</p>
    
    <img src="inter.png" alt="Sentiment" style="width: 100%; max-width: 600px; margin-top: 20px;">

    <iframe src="Spark-Mysql-Cassandra.html" width="100%" height="800px"></iframe>
</div>

<!-- Color band -->
<div class="colored-band"></div>

  <!-- project 2 section -->
<div class="w3-container w3-content w3-center w3-padding-64" style="max-width:800px" id="band_2">
  <h2 class="w3-wide">Classification model for predicting Popular songs</h2>
    <p class="w3-opacity"><i>Machine learning</i></p>
    <p class="w3-justify">Welcome to a transformative musical journey where data meets melody. In this project, I harnessed the power of 5 different machine learning models 
      (KNN, Naive Bayes, Random Forest, Decision tree, Support vector machine) to create predictive models for music popularity. By analyzing key features and
       patterns within a vast musical dataset, these models serve as a symphony of insights, forecasting the potential popularity of songs. Full code available here: <a href="https://github.com/alexCCTcollege/Music-classification/blob/main/hit%20song%20prediction.py" target="_blank">Github link</a>

      The feature of the datasets are: <br><br>
      
      -  Key: The key of a song refers to the tonal center or the pitch around which the music revolves. It is often identified by a note (e.g., C major, D minor).<br><br>
      
      -  Mode: Mode describes the overall mood or emotional quality of a piece of music. Common modes include major (happy or bright) and minor (sad or darker).<br><br>
      
      - Time Signature: This indicates the number of beats in a measure and which note value gets the beat. For example, 4/4 time signature means there are four beats 
      in a measure, and a quarter note gets one beat.<br><br>
      
      - Acousticness: A measure of how acoustic or electronic a track is. A higher value suggests a more acoustic (non-electronic) sound.<br><br>
      
      - Danceability: This metric assesses how suitable a track is for dancing based on its rhythm, tempo, and other musical elements.<br><br>
      
      - Energy: Represents the intensity and activity of a song. High-energy tracks are typically fast-paced and loud.<br><br>
      
      - Instrumentalness: Indicates whether a track is instrumental or contains vocals. A higher value suggests a higher likelihood of the track being instrumental.<br><br>
      
      - Liveness: This measures the likelihood that the recording is of a live performance. Higher values suggest a live recording.<br><br>
      
      - Loudness: The overall volume of the track. It is measured in decibels (dB).<br><br>
      
      - Speechiness: Reflects the presence of spoken words in a track. A higher value indicates more spoken words or a higher proportion of speech-like sounds.<br><br>
      
      - Valence: Describes the musical positiveness of a track. Tracks with high valence sound more positive or happy, while those with low valence sound more negative or sad.<br><br>
      
      - Tempo: The speed or pace of a piece of music, usually measured in beats per minute (BPM).<br><br>
      
      - Popularity: each song has been labeled as popular or non popular based on number of listeners.<br><br>

      After cleanining and processing Exploratory analysis was done in order to develop an understanding of trends and patterns for different genre of music. </p>
      
      <img src="music2.png" alt="Sentiment" style="width: 100%; max-width: 400px; margin-top: 20px;">

      <p class="w3-justify">The modelling has been developed by splitting the data into a 80/20 ratio of train and test data and results were evaluated (example in figure for KNN).</p>

      <img src="music4.png" alt="Sentiment" style="width: 100%; max-width: 400px; margin-top: 20px;">

      <p class="w3-justify">Each model has been tested for different genres.</p>
      
      <img src="music3.png" alt="Sentiment" style="width: 100%; max-width: 400px; margin-top: 20px;">

      <p class="w3-justify">Finally thanks to a confusion matrix the accuracy of the model have been tested.</p>

      <img src="music5.png" alt="Sentiment" style="width: 100%; max-width: 400px; margin-top: 20px;">

</div>

<div class="colored-band"></div>


<!-- project 3 section in case -->

<div class="w3-container w3-content w3-center w3-padding-64" style="max-width:800px" id="band_3">
  <h2 class="w3-wide">Meteorites dashboard in Tableau</h2>
    <p class="w3-opacity"><i>Data visualization</i></p>
    <div class='tableauPlaceholder' id='viz1701291017397' style='position: relative'><noscript><a href='#'><img alt='Meteorite landings ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Me&#47;Meteorites_16968079129650&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='Meteorites_16968079129650&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Me&#47;Meteorites_16968079129650&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1701291017397');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.width='1000px';vizElement.style.height='827px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='1000px';vizElement.style.height='827px';} else { vizElement.style.width='100%';vizElement.style.height='1027px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>

</div>

<!-- Color band -->
<div class="colored-band"></div>

<!-- project 4 section -->
<div class="w3-container w3-content w3-center w3-padding-64" style="max-width:800px" id="band_4">
  <h2 class="w3-wide">Up next: Neural networks, R project, Time series analysis, Python tutorials</h2>
    <p class="w3-opacity"><i>never stop learning</i></p>
</div>

<pre>
  #this is the file for hit song prediction on genre subset

  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns 
  from sklearn.metrics import confusion_matrix as confucio
  from sklearn.model_selection import train_test_split
  from collections import Counter
  from imblearn.under_sampling import RandomUnderSampler
  import numpy as np
  from sklearn.preprocessing import StandardScaler
  from sklearn.svm import SVC
  from imblearn.over_sampling import SMOTE  
  from imblearn.pipeline import Pipeline
  from sklearn.model_selection import GridSearchCV
  from sklearn.naive_bayes import GaussianNB
  from sklearn import tree
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.metrics import confusion_matrix
  from sklearn.metrics import accuracy_score
  from sklearn.metrics import f1_score
  from sklearn.metrics import precision_score
  from sklearn.metrics import recall_score
  from sklearn.model_selection import cross_val_score
  from sklearn.decomposition import PCA
  
  
  #DF
  df = pd.read_excel('C:/Users/santi/OneDrive/Desktop/df.xlsx')
  df.columns
  df['key'] = df['key'].astype('category',copy=False)
  df['mode'] = df['mode'].astype('category',copy=False)
  df['time_signature'] = df['time_signature'].astype('category',copy=False)
  regex = r'^(\d{4})' 
  df['year'] = df['release_date'].str.extract(regex, expand=False)
  df['year'] = pd.to_numeric(df['year'], errors='coerce')
  
  #ALBUM ROCK      
  album_rock = df[df['main_genre'] == 'album rock']
  album_rock = album_rock.drop(columns=['song_name', 'billboard', 
                  'artists', 'explicit','main_genre',
                  'release_date', 'song_id'])
  
      
  np.percentile(album_rock['popularity'], 75)
  plt.hist(album_rock['popularity'])
  plt.show()
  album_rock['popularity'].max()
  
  def cat(value):
      if value >= 54:
          return 1
      elif value < 54:
          return 0
     
  album_rock['pop'] = album_rock.apply(lambda row: cat(row['popularity']), axis = 1)
  album_rock = album_rock.drop(columns=['year'])
  album_rock = album_rock.drop(columns=['popularity'])
  album_rock = album_rock.drop(columns=['song_type'])
  
  
  #null and shape ckecks
  x_R = album_rock
  x_R.shape
  print(x_R.columns)
  sns.boxplot(data=x_R)
  plt.show()
  x_R.isna().sum()
  
  #define IQR
  IQR_list = []
  Q1_list = []
  Q3_list = []
  
  for gg in x_R.columns:
      Q1 = x_R[gg].quantile(0.25)
      Q3 = x_R[gg].quantile(0.75)
      IQR = Q3 - Q1
      IQR_list.append(IQR)
      Q1_list.append(Q1)
      Q3_list.append(Q3)
      
  #cancel outliers
  x_R = x_R[  x_R['duration_ms'] > (213127.0 - 1.5 * 70756.5)]
  x_R = x_R[  x_R['duration_ms'] < (283883.5 + 1.5 * 70756.5)]
  x_R = x_R[  x_R['acousticness'] > (0.0305 - 1.5 * 0.25949999999999995)]
  x_R = x_R[  x_R['acousticness'] < (0.29 + 1.5 * 0.25949999999999995)]
  x_R = x_R[  x_R['instrumentalness'] > (2.1e-06 - 1.5 * 0.0032979)]
  x_R = x_R[  x_R['instrumentalness'] < (0.0033 + 1.5 * 0.0032979)]
  x_R = x_R[  x_R['speechiness'] > (0.0305 - 1.5 * 0.0179)]
  x_R = x_R[  x_R['speechiness'] < (0.0484 + 1.5 * 0.0179)]
  
  #define target and categorical transformation
  yx_R =  x_R 
  categorical = ['key','mode', 'time_signature']
  x_R = pd.get_dummies(x_R, columns =categorical )
  y = yx_R['pop']
  x_R = x_R.drop(columns=['pop'])
  con_vars = ['duration_ms', 'acousticness','energy',
             'danceability', 'instrumentalness', 'liveness',
             'loudness', 'speechiness','valence','tempo']
  
  #scaler
  scaler = StandardScaler()
  x_R[con_vars]=scaler.fit_transform(x_R[con_vars])
  
  # PCA
  # Loop Function to identify number of principal components that explain at least 85% of the variance
  for comp in range(x_R.shape[1]):
      pca = PCA(n_components= comp, random_state=42)
      pca.fit(x_R)
      comp_check = pca.explained_variance_ratio_
      final_comp = comp
      if comp_check.sum() > 0.85:
          break
          
  Final_PCA = PCA(n_components= final_comp,random_state=42)
  Final_PCA.fit(x_R)
  cluster_df = Final_PCA.transform(x_R)
  num_comps = comp_check.shape[0]
  print("Using {} components, we can explain {}% of the variability in the original data.".format(final_comp,comp_check.sum()))
  
  
  
  precision_ = []
  recall_ = []
  accuracy_ = []
  f1_ = []
  
  '''KNN 1'''
  
  #split
  X_train, X_test, y_train, y_test = train_test_split(cluster_df,y,test_size=0.30)
  
  #undersampling
  print("Before undersampling: ", Counter(y_train))
  undersample = RandomUnderSampler(sampling_strategy='majority')
  X_train_under, y_train_under = undersample.fit_resample(X_train, y_train)
  print("After undersampling: ", Counter(y_train_under))
  
  #knn
  from sklearn.neighbors import KNeighborsClassifier
  knn = KNeighborsClassifier(n_neighbors=2)
  knn.fit(X_train_under, y_train_under)
  print(knn.score(X_test, y_test))
  
  
  #Checking for k value
  neighbors = np.arange(1, 9)
  train_accuracy = np.empty(len(neighbors))
  test_accuracy = np.empty(len(neighbors))
  
  # Loop over K values
  for i, k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_under, y_train_under)
    
    # Compute training and test data accuracy
    train_accuracy[i] = knn.score(X_train_under, y_train_under)
    test_accuracy[i] = knn.score(X_test, y_test)
  
  # Generate plot
  plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')
  plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')
  plt.legend()
  plt.xlabel('n_neighbors')
  plt.ylabel('Accuracy')
  plt.show()
  
  
  
  #cv
  #create a new KNN model
  knn_cv = KNeighborsClassifier(n_neighbors=2)
  #train model with cv of 5 
  cv_scores = cross_val_score(knn_cv, X_train_under, y_train_under, cv=5)
  #print each cv score (accuracy) and average them
  print(cv_scores)
  print('cv_scores mean:{}'.format(np.mean(cv_scores)))
  
  
  
  #confusion matrix
  y_pred= knn.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  
  #dance
  df['key'] = df['key'].astype('category',copy=False)
  df['mode'] = df['mode'].astype('category',copy=False)
  df['time_signature'] = df['time_signature'].astype('category',copy=False)
  regex = r'^(\d{4})' 
  df['year'] = df['release_date'].str.extract(regex, expand=False)
  df['year'] = pd.to_numeric(df['year'], errors='coerce')
  dance = df[df['main_genre'] == 'dance pop']
  dance = dance.drop(columns=['song_name', 'billboard', 
                  'artists', 'explicit','main_genre',
                  'release_date', 'song_id'])
  
  
  
  np.percentile(dance['popularity'], 75)
  plt.hist(dance['popularity'])
  plt.show()
  dance['popularity'].max()
  
  def cat2(value):
      if value >= 63:
          return 1
      elif value < 63:
          return 0
     
  dance['pop'] = dance.apply(lambda row: cat2(row['popularity']), axis = 1)
  dance = dance.drop(columns=['year'])
  dance = dance.drop(columns=['popularity'])
  dance = dance.drop(columns=['song_type'])
  
  #checks
  x_D = dance
  x_D.shape
  print(x_D.columns)
  sns.boxplot(data=x_D)
  plt.show()
  x_D.isna().sum()
  
  #IQR
  IQR_list = []
  Q1_list = []
  Q3_list = []
  
  for gg in x_D.columns:
      Q1 = x_D[gg].quantile(0.25)
      Q3 = x_D[gg].quantile(0.75)
      IQR = Q3 - Q1
      IQR_list.append(IQR)
      Q1_list.append(Q1)
      Q3_list.append(Q3)
      
  #handling outliers
  x_D = x_D[  x_D['duration_ms'] > (215707.0 - 1.5 * 53353)]
  x_D = x_D[  x_D['duration_ms'] < (269060.0 + 1.5 * 53353)]
  x_D = x_D[  x_D['acousticness'] > (0.0244 - 1.5 * 0.21509999999999999)]
  x_D = x_D[  x_D['acousticness'] < (0.2395 + 1.5 * 0.21509999999999999)]
  x_D = x_D[  x_D['instrumentalness'] > (0.0 - 1.5 * 4.6649999999999996e-05)]
  x_D = x_D[  x_D['instrumentalness'] < (4.6649999999999996e-05 + 1.5 * 4.6649999999999996e-05)] 
  x_D = x_D[  x_D['speechiness'] > (0.036449999999999996 - 1.5 * 0.061700000000000005)]
  x_D = x_D[  x_D['speechiness'] < (0.09815 + 1.5 * 0.061700000000000005)]
      
  #categorical data
  yx_D =  x_D 
  y = yx_D['pop']
  x_D = pd.get_dummies(x_D, columns =categorical )
  x_D = x_D.drop(columns=['pop'])
  
  
  '''KNN 2'''
  #scaler
  x_D[con_vars]=scaler.fit_transform(x_D[con_vars])
  
  # PCA
  # Loop Function to identify number of principal components that explain at least 85% of the variance
  for comp in range(x_R.shape[1]):
      pca = PCA(n_components= comp, random_state=42)
      pca.fit(x_D)
      comp_check = pca.explained_variance_ratio_
      final_comp = comp
      if comp_check.sum() > 0.85:
          break
          
  Final_PCA = PCA(n_components= final_comp,random_state=42)
  Final_PCA.fit(x_D)
  cluster_df2 = Final_PCA.transform(x_D)
  num_comps = comp_check.shape[0]
  print("Using {} components, we can explain {}% of the variability in the original data.".format(final_comp,comp_check.sum()))
  
  #split
  X_train, X_test, y_train, y_test = train_test_split(cluster_df2,y,test_size=0.30)
  
  #undersampling
  print("Before undersampling: ", Counter(y_train))
  undersample = RandomUnderSampler(sampling_strategy='majority')
  X_train_under, y_train_under = undersample.fit_resample(X_train, y_train)
  print("After undersampling: ", Counter(y_train_under))
  
  #knn
  knn = KNeighborsClassifier(n_neighbors=2)
  knn.fit(X_train_under, y_train_under)
  print(knn.score(X_test, y_test))
  
  
  #Checking for k value
  neighbors = np.arange(1, 9)
  train_accuracy = np.empty(len(neighbors))
  test_accuracy = np.empty(len(neighbors))
  
  # Loop over K values
  for i, k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_under, y_train_under)
    
    # Compute training and test data accuracy
    train_accuracy[i] = knn.score(X_train_under, y_train_under)
    test_accuracy[i] = knn.score(X_test, y_test)
  
  # Generate plot
  plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')
  plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')
  plt.legend()
  plt.xlabel('n_neighbors')
  plt.ylabel('Accuracy')
  plt.show()
  
  
  
  #knn
  knn_cv = KNeighborsClassifier(n_neighbors=2)
  cv_scores = cross_val_score(knn_cv, X_train_under, y_train_under, cv=5)
  print(cv_scores)
  print('cv_scores mean:{}'.format(np.mean(cv_scores)))
  
  
  
  y_pred= knn.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  #country 
  df = pd.read_excel('C:/Users/santi/OneDrive/Desktop/df.xlsx')
  df['key'] = df['key'].astype('category',copy=False)
  df['mode'] = df['mode'].astype('category',copy=False)
  df['time_signature'] = df['time_signature'].astype('category',copy=False)
  regex = r'^(\d{4})' 
  df['year'] = df['release_date'].str.extract(regex, expand=False)
  df['year'] = pd.to_numeric(df['year'], errors='coerce')
  cont = df[df['main_genre'] == 'contemporary country']
  cont = cont.drop(columns=['song_name', 'billboard', 
                  'artists', 'explicit','main_genre',
                  'release_date', 'song_id'])
  
  
  np.percentile(cont['popularity'], 75)
  plt.hist(cont['popularity'])
  plt.show()
  cont['popularity'].max()
  
  def cat3(value):
      if value >= 59:
          return 1
      elif value < 59:
          return 0
     
  cont['pop'] = cont.apply(lambda row: cat3(row['popularity']), axis = 1)
  cont = cont.drop(columns=['year'])
  cont = cont.drop(columns=['popularity'])
  cont = cont.drop(columns=['song_type'])
  
  
  x_C = cont
  x_C.shape
  print(x_C.columns)
  sns.boxplot(data=x_C)
  plt.show()
  x_C.isna().sum()
  
  #IQR
  IQR_list = []
  Q1_list = []
  Q3_list = []
  
  for gg in x_C.columns:
      Q1 = x_C[gg].quantile(0.25)
      Q3 = x_C[gg].quantile(0.75)
      IQR = Q3 - Q1
      IQR_list.append(IQR)
      Q1_list.append(Q1)
      Q3_list.append(Q3)
  
  #outliers
  x_C = x_C[  x_C['duration_ms'] > (198960.0 - 1.5 * 41800.0)]
  x_C = x_C[  x_C['duration_ms'] < (240760.0 + 1.5 * 41800.0)]
  x_C = x_C[  x_C['acousticness'] > (0.0459 - 1.5 * 0.2741)]
  x_C = x_C[  x_C['acousticness'] < (0.32 + 1.5 * 0.2741)]
  x_C = x_C[  x_C['instrumentalness'] > (0.0 - 1.5 * 2e-05)]
  x_C = x_C[  x_C['instrumentalness'] < (2e-05 + 1.5 * 2e-05)] 
  x_C = x_C[  x_C['speechiness'] > (0.0289 - 1.5 * 0.012899999999999998)]
  x_C = x_C[  x_C['speechiness'] < (0.0418 + 1.5 * 0.012899999999999998)]
      
  
  #categorical data
  yx_C =  x_C 
  y = yx_C['pop']
  x_C = pd.get_dummies(x_C, columns =categorical )
  x_C = x_C.drop(columns=['pop'])
  
  
  '''KNN 3'''
  x_C[con_vars] = scaler.fit_transform(x_C[con_vars])
  
  #PCA
  # Loop Function to identify number of principal components that explain at least 85% of the variance
  for comp in range(x_R.shape[1]):
      pca = PCA(n_components= comp, random_state=42)
      pca.fit(x_C)
      comp_check = pca.explained_variance_ratio_
      final_comp = comp
      if comp_check.sum() > 0.85:
          break
          
  Final_PCA = PCA(n_components= final_comp,random_state=42)
  Final_PCA.fit(x_C)
  cluster_df3 = Final_PCA.transform(x_C)
  num_comps = comp_check.shape[0]
  print("Using {} components, we can explain {}% of the variability in the original data.".format(final_comp,comp_check.sum()))
  
  #split
  X_train, X_test, y_train, y_test = train_test_split(cluster_df3,y,test_size=0.30)
  
  #undersampling
  print("Before undersampling: ", Counter(y_train))
  undersample = RandomUnderSampler(sampling_strategy='majority')
  X_train_under, y_train_under = undersample.fit_resample(X_train, y_train)
  print("After undersampling: ", Counter(y_train_under))
  
  #knn
  knn = KNeighborsClassifier(n_neighbors=2)
  knn.fit(X_train_under, y_train_under)
  print(knn.score(X_test, y_test))
  
  
  #Checking for k value
  neighbors = np.arange(1, 9)
  train_accuracy = np.empty(len(neighbors))
  test_accuracy = np.empty(len(neighbors))
  
  # Loop over K values
  for i, k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_under, y_train_under)
    
    # Compute training and test data accuracy
    train_accuracy[i] = knn.score(X_train_under, y_train_under)
    test_accuracy[i] = knn.score(X_test, y_test)
  
  # Generate plot
  plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')
  plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')
  plt.legend()
  plt.xlabel('n_neighbors')
  plt.ylabel('Accuracy')
  plt.show()
  
  
  
  #cv
  knn_cv = KNeighborsClassifier(n_neighbors=4)
  cv_scores = cross_val_score(knn_cv, X_train_under, y_train_under, cv=5)
  print(cv_scores)
  print('cv_scores mean:{}'.format(np.mean(cv_scores)))
  
  
  
  y_pred= knn.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  
  '''
  SVM
  '''
  #album rock
  y = yx_R['pop']
  
  X_train, X_test, y_train, y_test = train_test_split(cluster_df,y,test_size=0.30)
  
  #SVC Model
  model = Pipeline([
          ('balancing', SMOTE(random_state = 101)),
          ('classification', SVC(random_state=1) )])
  
  grid_params = {'classification__kernel': ['linear','poly', 'rbf', 'sigmoid'],
                'classification__C': [1,10,100]}
  
  svm_clf = GridSearchCV(estimator=model, param_grid=grid_params, scoring='precision_weighted', cv=5)
  # fit the model with the transformed training set
  svm_clf.fit(X_train,y_train)
  
  svm_clf_best_parameters = svm_clf.best_params_
  print("Optimal parameters:\n", svm_clf_best_parameters)
  
  svm_clf_best_result = svm_clf.best_score_ 
  print("Best mean cross-validated score:\n", svm_clf_best_result)
  
  y_pred= svm_clf.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  #dance
  x = x_D
  y = yx_D['pop']
  feature_scaler = StandardScaler()
  x_D = feature_scaler.fit_transform(x_D)
  X_train, X_test, y_train, y_test = train_test_split(cluster_df2,y,test_size=0.30)
  
  #SVC Model
  model = Pipeline([
          ('balancing', SMOTE(random_state = 101)),
          ('classification', SVC(random_state=1) )])
  
  grid_params = {'classification__kernel': ['linear','poly', 'rbf', 'sigmoid'],
                'classification__C': [1,10,100]}
  
  svm_clf = GridSearchCV(estimator=model, param_grid=grid_params, scoring='precision_weighted', cv=5)
  # fit the model with the transformed training set
  svm_clf.fit(X_train,y_train)
  
  svm_clf_best_parameters = svm_clf.best_params_
  print("Optimal parameters:\n", svm_clf_best_parameters)
  
  svm_clf_best_result = svm_clf.best_score_ 
  print("Best mean cross-validated score:\n", svm_clf_best_result)
  
  
  y_pred= svm_clf.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  #country
  x = x_C
  y = yx_C['pop']
  feature_scaler = StandardScaler()
  x_C = feature_scaler.fit_transform(x_C)
  X_train, X_test, y_train, y_test = train_test_split(cluster_df3,y,test_size=0.30)
  
  
  #SVC Model
  model = Pipeline([
          ('balancing', SMOTE(random_state = 101)),
          ('classification', SVC(random_state=1) )])
  
  grid_params = {'classification__kernel': ['linear','poly', 'rbf', 'sigmoid'],
                'classification__C': [1,10,100]}
  
  svm_clf = GridSearchCV(estimator=model, param_grid=grid_params, scoring='precision_weighted', cv=5)
  # fit the model with the transformed training set
  svm_clf.fit(X_train,y_train)
  
  svm_clf_best_parameters = svm_clf.best_params_
  print("Optimal parameters:\n", svm_clf_best_parameters)
  
  svm_clf_best_result = svm_clf.best_score_ 
  print("Best mean cross-validated score:\n", svm_clf_best_result)
  
  
  y_pred= svm_clf.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  
  
  
  '''
  NB
  '''
  #album rock
  
  y = yx_R['pop']
  
  X_train, X_test, y_train, y_test = train_test_split(cluster_df,y,test_size=0.30)
  nb_model = Pipeline([
          ('balancing', SMOTE(random_state = 101)),
          ('classification', GaussianNB())
      ])
  nb_model.get_params().keys()
  nb_clf = GridSearchCV(estimator=nb_model, param_grid={}, scoring='recall', cv=5)
  nb_clf.fit(X_train,y_train )
  
  
  y_pred = nb_clf.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  #dance
  y = yx_D['pop']
  feature_scaler = StandardScaler()
  x_D = feature_scaler.fit_transform(x_D)
  X_train, X_test, y_train, y_test = train_test_split(cluster_df2,y,test_size=0.30)
  nb_model = Pipeline([
          ('balancing', SMOTE(random_state = 101)),
          ('classification', GaussianNB())
      ])
  nb_model.get_params().keys()
  nb_clf = GridSearchCV(estimator=nb_model, param_grid={}, scoring='recall', cv=5)
  nb_clf.fit(X_train,y_train )
  
  
  y_pred= nb_clf.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  # country
  y = yx_C['pop']
  feature_scaler = StandardScaler()
  x_C = feature_scaler.fit_transform(x_C)
  X_train, X_test, y_train, y_test = train_test_split(cluster_df3,y,test_size=0.30)
  nb_model = Pipeline([
          ('balancing', SMOTE(random_state = 101)),
          ('classification', GaussianNB())
      ])
  nb_model.get_params().keys()
  nb_clf = GridSearchCV(estimator=nb_model, param_grid={}, scoring='recall', cv=75)
  nb_clf.fit(X_train,y_train )
  
  
  y_pred= nb_clf.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  
  
  
  
  '''
  tree
  '''
  #rock
  
  y = yx_R['pop']
  
  X_train, X_test, y_train, y_test = train_test_split(cluster_df,y,test_size=0.30)
  X_train_under, y_train_under = undersample.fit_resample(X_train, y_train)
  clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=3)
  clf = clf.fit(X_train_under, y_train_under)
  
  tree.plot_tree(clf)
  
  
  
  
  y_pred= clf.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  
  
  #dance
  y = yx_D['pop']
  feature_scaler = StandardScaler()
  x_D = feature_scaler.fit_transform(x_D)
  X_train, X_test, y_train, y_test = train_test_split(cluster_df2,y,test_size=0.30)
  X_train_under, y_train_under = undersample.fit_resample(X_train, y_train)
  
  clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=3)
  clf = clf.fit(X_train_under, y_train_under)
  
  tree.plot_tree(clf)
  
  y_pred= clf.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  
  #country
  y = yx_C['pop']
  feature_scaler = StandardScaler()
  x_C = feature_scaler.fit_transform(x_C)
  X_train, X_test, y_train, y_test = train_test_split(cluster_df3,y,test_size=0.30)
  X_train_under, y_train_under = undersample.fit_resample(X_train, y_train)
  
  
  clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=3)
  clf = clf.fit(X_train_under, y_train_under)
  
  tree.plot_tree(clf)
  
  
  
  y_pred= clf.predict(X_test)
  cm = confucio(y_test, y_pred)
  print(cm)
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  
  '''
  forest
  '''
  
  Smote = SMOTE()
  y = yx_R['pop']
  
  X_train, X_test, y_train, y_test = train_test_split(cluster_df,y,test_size=0.30)
  
  # summarize class distribution
  print("Before oversampling: ",Counter(y_train))
  
  # fit and apply the transform
  X_train_SMOTE, y_train_SMOTE = Smote.fit_resample( X_train , y_train)
  
  # summarize class distribution
  print("After oversampling: ",Counter(y_train_SMOTE))
  
  
  dt = RandomForestClassifier(random_state=42)
  
  params = {
      'max_depth': [2, 3, 5, 10, 20],
      'min_samples_leaf': [5, 10, 20, 50, 100],
      'criterion': ["gini", "entropy"]
  }
  grid_search = GridSearchCV(estimator=dt, 
                             param_grid=params, 
                             cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")
  
  grid_search.fit(X_train_SMOTE, y_train_SMOTE)
  score_df = pd.DataFrame(grid_search.cv_results_)
  score_df.head()
  score_df.nlargest(5,"mean_test_score")
  dt_best = grid_search.best_estimator_
  
  def evaluate_model(dt_classifier):
      print("Train Accuracy :", accuracy_score(y_train, dt_classifier.predict(X_train)))
      print("Train Confusion Matrix:")
      print(confusion_matrix(y_train, dt_classifier.predict(X_train)))
      print("-"*50)
      print("Test Accuracy :", accuracy_score(y_test, dt_classifier.predict(X_test)))
      print("Test Confusion Matrix:")
      print(confusion_matrix(y_test, dt_classifier.predict(X_test)))
      
  evaluate_model(dt_best)
  
  
  y_pred= grid_search.predict(X_test)
  
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  #dance
  y = yx_D['pop']
  
  X_train, X_test, y_train, y_test = train_test_split(cluster_df2,y,test_size=0.30)
  
  # summarize class distribution
  print("Before oversampling: ",Counter(y_train))
  
  # fit and apply the transform
  X_train_SMOTE, y_train_SMOTE = Smote.fit_resample( X_train , y_train)
  
  # summarize class distribution
  print("After oversampling: ",Counter(y_train_SMOTE))
  
  
  dt = RandomForestClassifier(random_state=42)
  
  params = {
      'max_depth': [2, 3, 5, 10, 20],
      'min_samples_leaf': [5, 10, 20, 50, 100],
      'criterion': ["gini", "entropy"]
  }
  grid_search = GridSearchCV(estimator=dt, 
                             param_grid=params, 
                             cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")
  
  grid_search.fit(X_train_SMOTE, y_train_SMOTE)
  score_df = pd.DataFrame(grid_search.cv_results_)
  score_df.head()
  score_df.nlargest(5,"mean_test_score")
  dt_best = grid_search.best_estimator_
  
  evaluate_model(dt_best)
  
  y_pred= grid_search.predict(X_test)
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  #country
  y = yx_C['pop']
  
  X_train, X_test, y_train, y_test = train_test_split(cluster_df3,y,test_size=0.30)
  
  # summarize class distribution
  print("Before oversampling: ",Counter(y_train))
  
  # fit and apply the transform
  X_train_SMOTE, y_train_SMOTE = Smote.fit_resample( X_train , y_train)
  
  # summarize class distribution
  print("After oversampling: ",Counter(y_train_SMOTE))
  
  
  dt = RandomForestClassifier(random_state=42)
  
  params = {
      'max_depth': [2, 3, 5, 10, 20],
      'min_samples_leaf': [5, 10, 20, 50, 100],
      'criterion': ["gini", "entropy"]
  }
  grid_search = GridSearchCV(estimator=dt, 
                             param_grid=params, 
                             cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")
  
  grid_search.fit(X_train_SMOTE, y_train_SMOTE)
  score_df = pd.DataFrame(grid_search.cv_results_)
  score_df.head()
  score_df.nlargest(5,"mean_test_score")
  dt_best = grid_search.best_estimator_
      
  evaluate_model(dt_best)
  
  y_pred= grid_search.predict(X_test)
  
  print(precision_score(y_test, y_pred, average='macro'))
  print(recall_score(y_test, y_pred, average='macro'))
  print(accuracy_score(y_test, y_pred))
  print(f1_score(y_test, y_pred , average = 'binary'))
  
  
  precision_.append(precision_score(y_test, y_pred, average='macro'))
  recall_.append(recall_score(y_test, y_pred, average='macro'))
  accuracy_.append(accuracy_score(y_test, y_pred))
  f1_.append(f1_score(y_test, y_pred , average = 'binary'))
  
  
  print(precision_)
  print(recall_)
  print(accuracy_)
  print(f1_)
  
  
  
  print((sum(precision_)/len(precision_)))
  print((sum(recall_)/len(recall_)))
  print((sum(accuracy_)/len(accuracy_)))
  print((sum(f1_)/len(f1_)))
  

</pre>
</script>

</body>
</html>